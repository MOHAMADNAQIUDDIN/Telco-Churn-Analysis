# -*- coding: utf-8 -*-
"""2.  Machine Learning (Model Building) [TELCO CHURN]

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QIY7aeFALqWAOYEG4DRXnrDlRQDLRTWX
"""

import pandas as pd

import warnings
warnings.filterwarnings("ignore")

"""## Load Data"""

telco_base_data = pd.read_csv("/content/WA_Fn-UseC_-Telco-Customer-Churn.csv")

churn_data = pd.read_csv('/content/Scaled_Churn_data.csv')

churn_data.head()

churn_data.isnull().sum()

churn_data.dropna(inplace=True)

"""**Divide Data into Features (X) and Target (y)**"""

X = churn_data.drop('Churn',axis=1)

y = churn_data['Churn']

"""**Split Training And Test Data**"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=0, test_size=0.20)

"""**Select Algorithm**"""

from sklearn.tree import DecisionTreeClassifier

model_dt = DecisionTreeClassifier()

"""**Train Model with Algorithm + Train Data**"""

model_dt.fit(X_train,y_train)

"""**Test Model with Test Dataset**"""

y_pred = model_dt.predict(X_test)

"""**Check Model Accuracy with Performance Matrix**"""

from sklearn.metrics import confusion_matrix

conf_Matrix = confusion_matrix(y_test, y_pred)
print(conf_Matrix)

from sklearn.metrics import classification_report

print (classification_report(y_test, y_pred))

"""Very low accuracy and also low score for Detecting Churn in F1, Precision and Recall.<br>
Reason: Imbalance Target 
"""

telco_base_data['Churn'].value_counts()

"""**If we've imbalanced data we need upSample our Data** <br>
Method : SMOTE-ENN <br>
It aims to balance class distribution by randomly increasing minority class examples by replicating them. 
SMOTE synthesizes new minority instances between existing minority instances. It generates the virtual training records by linear interpolation for the minority class.

**if imblearn not installed in your machine. Then execute following snippet in jupyter notebook**

```
!pip install imblearn
```
"""

from imblearn.combine import SMOTEENN

sm = SMOTEENN()

X_resample, y_resample = sm.fit_resample(X,y)

X_train, X_test, y_train, y_test = train_test_split(X_resample, y_resample, random_state=0, test_size = 0.20)
model_dt = DecisionTreeClassifier()
model_dt.fit(X_train,y_train)
y_pred = model_dt.predict(X_test)
print (classification_report(y_test, y_pred))

"""**Random Forest Classifier**"""

from sklearn.ensemble import RandomForestClassifier

"""**performance Matrix of Classification Problem** <br>
https://medium.com/@MohammedS/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b
"""

model_RF = RandomForestClassifier()
model_RF.fit(X_train,y_train)
y_pred = model_RF.predict(X_test)
print (classification_report(y_test, y_pred))

"""**Save the Best Model**"""

import pickle

pickle.dump(model_RF,open("Model_RF.sav",'wb')) #Write-Binary

"""**Read Model**"""

load_my_model = pickle.load(open("Model_RF.sav",'rb')) # Read-Binary

y_my_pred = load_my_model.predict(X_test)

print (classification_report(y_test, y_pred))

"""### Build a Model Selecting Best Features"""

best_features = ['SeniorCitizen', 'tenure', 'Contract','MonthlyCharges', 'TotalCharges', 'Dependents',
       'MultipleLines', 'InternetService','OnlineSecurity','OnlineBackup','TechSupport','PaperlessBilling',
       'PaymentMethod']

len(best_features)

len(telco_base_data.columns)

df = telco_base_data[best_features]

df.head()

df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')

df.dropna(inplace=True)

label = ["{}-{}".format(i,i+11) for i in range(1,72,12) ]
df['tenure_group'] = pd.cut(df['tenure'],range(1,80,12),labels=label,right=False)

df.drop('tenure',axis=1,inplace=True)

df.head()

df_dummies = pd.get_dummies(df)

from sklearn.preprocessing import StandardScaler

df_scaled = StandardScaler().fit_transform(df_dummies)

df_scaled_df = pd.DataFrame(df_scaled, columns=df_dummies.columns)

target = telco_base_data['Churn'].apply(lambda label: 1 if label=='Yes' else 0)

final_df = pd.concat([df_scaled_df,target],axis=1)

final_df.head()

final_df.dropna(inplace=True)

X = final_df.drop('Churn',axis=1)
y = final_df['Churn']

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from imblearn.combine import SMOTEENN

sm = SMOTEENN()
X_resample,y_resample = sm.fit_sample(X,y)
X_train, X_test, y_train, y_test = train_test_split(X_resample, y_resample,random_state=0, test_size=0.20)

model_rf = RandomForestClassifier()
model_rf.fit(X_train,y_train)
y_pred = model_rf.predict(X_test)
print(classification_report(y_test, y_pred , labels=[0,1]))

import pickle
pickle.dump(model_rf,open("Best_Model.pkl",'wb')) # Write-Binary

"""### For Batch Data Inference"""

df_1 = pd.read_csv("/content/first_telc_up.csv")

df_1.head()

df_1['TotalCharges'] = pd.to_numeric(df_1['TotalCharges'], errors='coerce')

label = ["{}-{}".format(i,i+11) for i in range(1,72,12) ]
df_1['tenure_group'] = pd.cut(df_1['tenure'],range(1,80,12),labels=label,right=False)

df_1.drop('tenure',inplace=True,axis=1)

#Convert to Dummy
df_dummy = pd.get_dummies(df_1)

# Scaling Features
feature_scaled = StandardScaler().fit_transform(df_dummy)
final_df_scaled = pd.DataFrame(feature_scaled, columns=df_dummy.columns)

final_df_scaled.dropna(inplace=True)

def prediction_batch(data):
  y_pred = model_rf.predict(data)
  return y_pred

churn_pred = pd.DataFrame(prediction_batch(final_df_scaled))

churn_pred.columns = ['Churn_pred']

churn_pred

df_1.dropna(inplace=True)

pd.concat([df_1,churn_pred],axis=1)

single = model.predict(final_df_scaled.tail(1))[0]
probablity = model.predict_proba(final_df_dummy.tail(1))[0,1]